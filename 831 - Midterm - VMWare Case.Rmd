---
title: "831 Midterm - VMWare Case Study"
output: html_notebook
---

```{r}
library(dplyr)
library(tidyverse)
pacman::p_load("caret","ROCR","lift","randomForest", "ranger","MASS","e1071", "pROC", "MLmetrics", "DMwR", "cvAUC")  #Check, and if needed install the necessary packages
```
VMware provides cloud computing and virtualization software and services. We are provided with customer data pertaining to Online Engagement, CRM Records, and Digital Activity and asked to design a Propensity to Respond classification model. In this exercise, our main challenges are:  
1. Dealing with imbalanced data  
2. Multiclass and Binary Classification  
3. Exploring Ensemble Algorithms (Random Forest, XGBoost, and Committees)  
4. Evaluating the algorithms using Log Loss  

## Data Understanding
### Load the data
We are presented with two files: Training.csv and Validation.csv. Each file contains 706 features and 50,006 instances.
```{r}
train <-read.csv("C:\\Users\\jdonv\\OneDrive - Queen's University\\MMA831 - Marketing Analytics\\Assignment 1\\IMB 623 VMWare- Digital Buyer Journey\\Training.csv", na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
```

```{r}
holdout <-read.csv("C:\\Users\\jdonv\\OneDrive - Queen's University\\MMA831 - Marketing Analytics\\Assignment 1\\IMB 623 VMWare- Digital Buyer Journey\\Validation.csv", na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
```

```{r}
train$Set <- "Training"
holdout$Set <- "Validation"
```

Combine the datasets for exploration and preprocessing  
```{r}
df <- rbind(train, holdout)
```


### Data Overview
#### Preview the data
```{r}
head(df)
```

Some dataframes contain only 1 value - this information is not useful for our model. Before we drop them, what are they?

```{r}
names(df[sapply(df, function(x) length(unique(x))>1)==FALSE])
```
```{r}
df <- df[sapply(df, function(x) length(unique(x))>1)]
```

Since we have ~600 features remaining, the standard str function will be hard to look at. I use below code to instead and created a dataframe to better understand the data.
```{r}
struc <- data.frame(variable = names(df),
           class = sapply(df, class),
           first_values = sapply(df, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) 

missingvals <- data.frame(sapply(df, function(x) sum(is.na(x))))               
missingvals <- cbind(variable = row.names(missingvals), missingvals)
missingvals <- rename(missingvals, missing = 2)

summary_df <- merge(struc, missingvals)
summary_df <- summary_df %>% 
  mutate(percent_missing = missing/nrow(df))

numlevels <- data.frame(sapply(df, nlevels))
numlevels <- cbind(variable = row.names(numlevels), numlevels)
numlevels <- rename(numlevels, levels = 2)

summary_df <- merge(summary_df, numlevels)
```

Let's look at the number of fields by data type:
```{r}
summary_df %>%
  group_by(class) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% arrange(desc(freq))
```

Most of the fields are numeric. Some are factors.  

Another observation, is that some of the fields that describe days between customer actions, have 9999s as placeholders. We choose to leave these here and let the model decide the difference between smaller "tangible" values between 0 and 100, and the 9999 placeholders.

#### Target
What about the target? The target needs to be recoded as a factor, to recognize it this as classification and not regression in our models.
```{r}
df$target <- as.factor(df$target)
```

Distribution of the target  
Let's explore the target. From below, we can see that 97.26% of the instances belong to class 0. If we predict 0 for all outcomes then we will have an accuracy of 97.26%. We'll keep this in mind.Additionally, with target 2 and 3, there may not be enough signal in general to identify these outcomes.
```{r}
plot(df$target)
```

```{r}
df %>%
  group_by(target) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```


## Data Preparation
### Missing Values
We have many features with missing values. We need to be able to apply mass cleaning functions to them since there are so many. I look at features that have more than 25% of data missing, and those with less.  

#### More than 25% missing
```{r}
summary_df %>% filter(percent_missing > 0.25)
```

#### Less than 25% missing
```{r}
summary_df %>% filter(percent_missing <= 0.25 & percent_missing > 0 )
```

#### Imputation of Missing Values Plan
We grouped features with missing data into two groups: less than 25% missing and more than 25% missing.  
  
Missing values > 25%  
•	For those features beginning with "channel_" and ending with “booking_amount”, they looked like they are dollar amounts, so we replaced the n/as replace with 0.  
•	Change features beginning with "ftr_first_date" into dummies, since majority are missing and most of the dates are different from each other.   
•	All other features were dropped, since they have high cardinality and a lot of missing data. We have no way of imputing.  
Missing Values <= 0.25  
•	There are a few factors that did not have too much missing data. We encoded the n/as as "Unknown".  
  
First, addressing the channel totals and booking amounts:
```{r}
vars_0 <- summary_df %>% filter(percent_missing > 0.25 & (str_detect(variable, "^channel") | str_detect(variable, "booking_amount$") ) ) %>% dplyr::select(variable)

varlist <- c(vars_0)
#https://stackoverflow.com/questions/43839243/error-invalid-subscript-type-list-in-r
varlist <- as.vector(unlist(vars_0))
df[, varlist][is.na(df[, varlist])] <- 0
```

Dealing with ftr and db variables  
```{r}
vars_ftr <- summary_df %>% filter((percent_missing > 0.25 & (str_detect(variable, "^ftr")) | percent_missing <= 0.25 & percent_missing > 0   ) ) %>% dplyr::select(variable)

#https://stackoverflow.com/questions/43839243/error-invalid-subscript-type-list-in-r
vars_ftr <- as.vector(unlist(c(vars_ftr)))

df <- mutate_at(df, vars(vars_ftr), as.character)

df[, vars_ftr][is.na(df[, vars_ftr])] <- "Unknown"
df <- mutate_at(df, vars(vars_ftr), as.factor)
```

Creating dummies for the date fields  
```{r}
df$ftr_first_date_any_dummy <- ifelse(df$ftr_first_date_any_download != "Unknown", 1, 0)
df$ftr_first_date_white_dummy <- ifelse(df$ftr_first_date_whitepaper_download != "Unknown", 1, 0)
df$ftr_first_date_hol_dummy <- ifelse(df$ftr_first_date_hol_page_view != "Unknown", 1, 0)
df$ftr_first_date_eval_dummy <- ifelse(df$ftr_first_date_eval_page_view != "Unknown", 1, 0)
df$ftr_first_date_web_dummy <- ifelse(df$ftr_first_date_webinar_page_view != "Unknown", 1, 0)
```

Drop the rest of the features that we did not intend to impute from the dataframe  
```{r}
vars_drop <- summary_df %>% filter(percent_missing > 0.25 & ((str_detect(variable, "^channel", negate=TRUE) & 
                                                   str_detect(variable, "booking_amount$", negate=TRUE)))) %>% dplyr::select(variable)
```

```{r}
vars_drop <- as.vector(unlist(c(vars_drop)))
myexprs <- purrr::map( vars_drop, rlang::parse_expr )
df <- df %>% dplyr::select(!c( !!!myexprs ))
```

Let's double check the data has no more missing values before proceeding. Below code shows nothing, great!  
```{r}
missingvalscheck <- data.frame(sapply(df, function(x) sum(is.na(x))))               
missingvalscheck <- cbind(variable = row.names(missingvalscheck), missingvalscheck)
missingvalscheck <- rename(missingvalscheck, missing = 2)
missingvalscheck %>% filter(missing > 0)

```

Combine Rare Categories for Factors  
```{r}
df_target <- df$target
df_else <- df %>% dplyr::select(-c(target))

combinerarecategories<-function(data_frame,mincount){ 
  for (i in 1 : ncol(data_frame)){
    a<-data_frame[,i]
    replace <- names(which(table(a) < mincount))
    levels(a)[levels(a) %in% replace] <-paste("Other",colnames(data_frame)[i],sep=".")
    data_frame[,i]<-a }
  return(data_frame) }


#Apply combinerarecategories function to the data and then split it into testing and training data.

df_else<-combinerarecategories(df_else,100) #combine categories with <10 values in STCdata into "Other"
df <- cbind(df_else,target=df_target)
```

Drop the X feature, which is just the row number of the instance  
```{r}
df <- df %>% dplyr::select(-c(X))
```

## Modelling

### Baseline Model & Removal of Target Leakage 
To create a baseline measurement, we split the “Training.csv” dataset into a 70% training and 30% test set. We trained a Random Forest model on the training set and measured the output against the test set. Our initial observations were that the accuracy (0.9898), AUC (0.9972), and Log Loss (0.033) were too high and we suspected target leakage. 

Create dummies
```{r}
df_dummies <- model.matrix(target~ ., data = df)[,-1]
df_dummies<-as.data.frame(df_dummies)
```

Train Test Split
```{r}
set.seed(77850) #set a random number generation seed to ensure that the split is the same everytime

train_df <- filter(df_dummies, SetValidation == 0) %>% dplyr::select(-SetValidation) #Full Training/Testing set
validation_df <- filter(df_dummies, SetValidation == 1) %>% dplyr::select(-SetValidation) #Holdout Set
train_df_y <- filter(df, Set=="Training") %>% dplyr::select(target) #Holdout Set
validation_df_y <-filter(df, Set=="Validation") %>% dplyr::select(target) #Holdout Set

```

```{r}
inTrain <- createDataPartition(y = train_df_y$target,
                               p = 0.70, list = FALSE)

x_train <-train_df[ inTrain,]
x_test <- train_df[ -inTrain,]
y_train <-train_df_y[ inTrain,]
y_test <-train_df_y[ -inTrain,]

```

```{r}
#https://stackoverflow.com/questions/42522535/using-ranger-method-caret-train-function
#https://stats.stackexchange.com/questions/458140/advice-on-running-random-forests-on-a-large-dataset

fit <- ranger(x = x_train, y = y_train,
              num.trees = 500,
              max.depth = 8,
              probability = TRUE, importance="impurity", seed=1)

```

```{r}
predictions <- as.data.frame(predict(fit, x_test, type = "response", verbose=TRUE)$predictions)
predict_labels <- as.factor(max.col(predictions)-1)

# Accuracy and other metrics
confusionMatrix(predict_labels,y_test)
```
AUC
```{r}
multiclass.roc(y_test, predictions)
```

LogLoss
```{r}
#https://www.rdocumentation.org/packages/MLmetrics/versions/1.1.1/topics/MultiLogLoss
MultiLogLoss(y_true = y_test, y_pred = predictions)
```

The accuracy is 98.98% and the AUC is 99.72%. It looks like we could have a problem with target leakage. The target can be inferred with (near) perfect collinearity to some of the features.
```{r}
  feat_imp_df <- importance(fit) %>% 
    data.frame() 

feat_imp_df$variable <- rownames(feat_imp_df)
feat_imp_df <- feat_imp_df %>% arrange(desc(.))
```

```{r}
head(feat_imp_df,20)
```

```{r}
drop_tgt_leak <- summary_df %>% filter(str_detect(variable, "^tgt")) %>% dplyr::select(variable)
drop_tgt_leak <- as.vector(unlist(c(drop_tgt_leak)))
myexprs <- purrr::map( drop_tgt_leak, rlang::parse_expr )
```

Pearson Correlation between Binarized Target and Tgt fields shows high correlation. There's less correlation on some of the fields, but that may be because the target ranges between 0-5.
```{r}
#https://business-science.github.io/correlationfunnel/reference/correlate.html
library(correlationfunnel)
corr_df <- df %>% dplyr::select(c( !!!myexprs ))
y_binary_all <- as.numeric(ifelse(df$target != 0, 1, 0))
corr_df <- cbind(corr_df, target=y_binary_all)

corr_df %>%
  correlate(target)

```
Pearson Correlation between Binarized Target and download date fields also shows high correlation. 
```{r}

corr_dummies_df <- df %>% dplyr::select(c(ftr_first_date_any_dummy, ftr_first_date_white_dummy, ftr_first_date_hol_dummy,ftr_first_date_eval_dummy, ftr_first_date_web_dummy, masked_email))
corr_dummies_df <- cbind(corr_dummies_df, target=y_binary_all)

corr_dummies_df %>%
  correlate(target)
```

I removed the ftr_date dummies, tgt_ features, and masked_email features.Although some of the features appear to have less correlation to a binarized target, we played it safe and removed all of them, due to the semantic implications of their names with how they might be related to the target.
```{r}
df <- df %>% dplyr::select(!c( !!!myexprs ))
df <- df %>% dplyr::select(!c(ftr_first_date_any_dummy, ftr_first_date_white_dummy, ftr_first_date_hol_dummy,ftr_first_date_eval_dummy, ftr_first_date_web_dummy, masked_email))
```

We re-run the baseline model to be sure.
#### Baseline Model Rerun
Train Test Split
```{r}
df_dummies <- model.matrix(target~ ., data = df)[,-1]
df_dummies<-as.data.frame(df_dummies)
train_df <- filter(df_dummies, SetValidation == 0) %>% dplyr::select(-SetValidation) #Full Training/Testing set
validation_df <- filter(df_dummies, SetValidation == 1) %>% dplyr::select(-SetValidation) #Holdout Set
train_df_y <- filter(df, Set=="Training") %>% dplyr::select(target) #Holdout Set
validation_df_y <-filter(df, Set=="Validation") %>% dplyr::select(target) #Holdout Set
x_train <-train_df[ inTrain,]
x_test <- train_df[ -inTrain,]
y_train <-train_df_y[ inTrain,]
y_test <-train_df_y[ -inTrain,]
```

```{r}
fit <- ranger(x = x_train, y = y_train,
              num.trees = 500,
              max.depth = 8,
              probability = TRUE, importance="impurity", seed=1)
```

```{r}
predictions_2 <- as.data.frame(predict(fit, x_test, type = "response", verbose=TRUE)$predictions)
predict_labels_2 <- as.factor(max.col(predictions_2)-1)

# Accuracy and other metrics
confusionMatrix(predict_labels_2,y_test)
```
```{r}
multiclass.roc(y_test, predictions_2)
```

```{r}
#https://www.rdocumentation.org/packages/MLmetrics/versions/1.1.1/topics/MultiLogLoss
MultiLogLoss(y_true = y_test, y_pred = predictions_2)
```


We've lost considerable sensitivity in each of the classes, have a lower accuracy and lower AUC. But this makes more sense. If we have more instances for the minority classes, how well does this do on the holdout set? We'll use hyperparameter tuning on the training set, and test on the holdout set.

### 3.2.	Hyperparameter Tuning, Cross Validation and Testing on the Validation Dataset
The Hands-on Machine Learning with R book is a good reference (https://bradleyboehmke.github.io/HOML/random-forest.html#rf-tuning-strategy)  
  
For hyperparameter tuning, we are now training the model on the entire “Training” dataset and testing the performance on the holdout (“Validation”) dataset. We use the caret package to test different hyperparameters, optimizing for Log Loss, and using 5-fold cross validation. We are using Log Loss because it penalizes the models for being more confident about wrong predictions, which will help for this imbalanced multi-class problem.  

Our grid included below hyperparameter options:  
•	mtry: Literature suggests a default value equal to the square root of the # features (~ 25). We will try 20, 25, 30  
•	min node size: performance may improve by increasing node size for noisy data. We will try 10, 15, 20  
•	split rule: we will stick to gini  
The resulting model selected mtry = 20, split = gini, and min.node.size = 20. The average CV Log Loss was 0.1240. The Validation Log Loss was 0.1456.  

```{r}
#https://stackoverflow.com/questions/18402016/error-when-i-try-to-predict-class-probabilities-in-r-caret
train_df_y <- train_df_y %>% 
  mutate(target = factor(target, 
          labels = make.names(levels(target))))
```

```{r}
#https://topepo.github.io/caret/measuring-performance.html -> Suggests mutliclass summary function gives other info as well
#grid <-  expand.grid(mtry = c(30), splitrule = c("gini"), min.node.size = c(20))
grid <-  expand.grid(mtry = c(20, 25, 30), splitrule = c("gini"), min.node.size = c(10, 15, 20))

fitControl <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE,
                           classProbs = TRUE,
                           summaryFunction=multiClassSummary )

fit_training = train(
  x = train_df,
  y = as.factor(train_df_y$target),
  method = 'ranger',
  metric = 'logLoss',
  num.trees = 500,
  tuneGrid = grid,
  trControl = fitControl
)
print(fit_training)

```

Evaluating the overall accuracy, shows 97.34%, however, the sensitivty in each of the classes is quite poor.

```{r}
validation_df_y <- validation_df_y %>% 
  mutate(target = factor(target, 
          labels = make.names(levels(target))))
```

This model is not predicting any of target 2 to 5, so at this point, we decided to try an up sampling model. The average CV Log Loss was 0.1456. The Validation Log Loss was 0.1633. The resulting confusion matrix is below:

```{r}
predict_training <- predict(fit_training, validation_df)

# Accuracy and other metrics
confusionMatrix(predict_training,as.factor(validation_df_y$target))
```

```{r}
val_df_y_num <- as.numeric(substr(validation_df_y$target,2,2))
pred_train_y_num <- as.numeric(substr(predict_training,2,2))
```

```{r}
predict_training_probs <- predict(fit_training, validation_df, type="prob")
```

```{r}
#https://www.rdocumentation.org/packages/MLmetrics/versions/1.1.1/topics/MultiLogLoss
MultiLogLoss(y_true = validation_df_y$target, y_pred = as.matrix(predict_training_probs))
```
```{r}
multiclass.roc(val_df_y_num, pred_train_y_num)
```
### Correcting Class Imbalance with Up-Sampling
```{r}
grid_up <-  expand.grid(mtry = c(30), splitrule = c("gini"), min.node.size = c(10))
fitControl_up <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE, sampling="up",
                           classProbs = TRUE,
                           summaryFunction=multiClassSummary)

fit_training_up = train(
  x = train_df,
  y = as.factor(train_df_y$target),
  method = 'ranger',
  metric = 'logLoss',
  num.trees = 500,
  tuneGrid = grid_up,
  trControl = fitControl_up
)
print(fit_training_up)


```
We can see that with up sampling, it did not really fare much better. To address the class imbalance, we changed the target into a binary (0 = no response, 1 = any response) value.

```{r}
predict_training_up <- predict(fit_training_up, validation_df)

# Accuracy and other metrics
confusionMatrix(predict_training_up,validation_df_y$target)
```

```{r}
predict_training_up_probs <- predict(fit_training_up, validation_df, type="prob")
```

Log Loss is worse.
```{r}
MultiLogLoss(y_true = validation_df_y$target, y_pred = as.matrix(predict_training_up_probs))
```
```{r}
pred_train_y_up_num <- as.numeric(substr(predict_training_up,2,2))
multiclass.roc(val_df_y_num, pred_train_y_up_num)
```


### Changing from Multiclass to Binary
Two models were run, one binary model without balancing, and one with up sampling to correct the class imbalance. Both models were tuned with the same grid of hyperparameters outlined earlier. Although the log loss scores were better for the binary model without balancing, when reviewing the confusion matrices, the sensitivity of the up sampled model was better (0.1345 vs 0.1025).

```{r}
train_df_y_bin <- as.factor(ifelse(train_df_y != "X0", "X1", "X0"))
validation_df_y_bin <-as.factor(ifelse(validation_df_y != "X0", "X1", "X0"))

```

```{r}
levels(train_df_y_bin) <- c("no_response", "response")
levels(validation_df_y_bin) <- c(0, 1)
```

```{r}
levels(validation_df_y_bin)
```
```{r}
#https://stackoverflow.com/questions/45333029/specifying-positive-class-of-an-outcome-variable-in-caret-train
train_df_y_bin <- factor(train_df_y_bin, levels=rev(levels(train_df_y_bin)))
validation_df_y_bin <- factor(validation_df_y_bin, levels=rev(levels(validation_df_y_bin)))
```


```{r}
#grid_b <-  expand.grid(mtry = c(30), splitrule = c("gini"), min.node.size = c(15))
grid_b <-  expand.grid(mtry = c(25, 30), splitrule = c("gini"), min.node.size = c(10, 15, 20))
fitControl_b <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE, classProbs=TRUE,
                           summaryFunction=multiClassSummary)

fit_training_b = train(
  x = train_df,
  y = train_df_y_bin,
  method = 'ranger',
  metric = 'logLoss',
  num.trees = 500,
  tuneGrid = grid_b,
  trControl = fitControl_b,
  importance='impurity'
)
print(fit_training_b)


```
```{r}
varImp(fit_training_b)
```


```{r}
predict_binary <- predict(fit_training_b, validation_df, type = "prob", verbose=TRUE)$response
predict_binary_0.5 <- as.factor(ifelse(predict_binary>0.5,1,0))
confusionMatrix(predict_binary_0.5,validation_df_y_bin)
```
Adjust the decision threshold downward
```{r}
predict_binary_0.15 <- as.factor(ifelse(predict_binary>0.15,1,0))
confusionMatrix(predict_binary_0.15,validation_df_y_bin)
```


```{r}
library(varhandle)
y_bin_unf <- unfactor(validation_df_y_bin)
```

```{r}
LogLoss(y_true = y_bin_unf, y_pred =predict_binary)
```


#### AUC

```{r}
forest_prediction <- predict(fit_training_b, validation_df, type = "prob", verbose=TRUE)$response
bin_auc <- AUC(forest_prediction, validation_df_y_bin)
bin_auc
#change the ordering https://www.rdocumentation.org/packages/cvAUC/versions/1.1.0/topics/AUC
```
```{r}
plot.roc(validation_df_y_bin, forest_prediction)
```


#### Tuning the threshold
By lowering the decision threshold, we can increase the sensitivity of the model, at the expense of precision
```{r}
thresholds <- seq(0.1, 0.90, by=0.1)

for (i in thresholds) {
  print(i)
  predict_binary_i <- as.factor(ifelse(predict_binary>i,1,0))
  print(confusionMatrix(predict_binary_i,validation_df_y_bin))
  
}
```

#### Precision Recall Curve
```{r}
library(PRROC)
#https://stackoverflow.com/questions/25020788/in-r-calculate-area-under-precision-recall-curve-aupr


pr <- pr.curve(scores.class0=predict_binary[validation_df_y_bin==1],
             scores.class1=predict_binary[validation_df_y_bin==0],
             curve=T)
pr
```


```{r}
plot(pr)
```

### Binary Up Sampling Model
```{r}
#grid_b <-  expand.grid(mtry = c(30), splitrule = c("gini"), min.node.size = c(15))
grid_b <-  expand.grid(mtry = c(25, 30), splitrule = c("gini"), min.node.size = c(10, 15, 20))
fitControl_b_up <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE, classProbs=TRUE,
                           summaryFunction=multiClassSummary, sampling="up")

fit_training_b_up = train(
  x = train_df,
  y = train_df_y_bin,
  method = 'ranger',
  metric = 'logLoss',
  num.trees = 500,
  tuneGrid = grid_b,
  trControl = fitControl_b_up,
  importance='impurity'
)
print(fit_training_b_up)


```

```{r}
predict_binary_up <- predict(fit_training_b_up, validation_df, type = "prob", verbose=TRUE)$response
predict_binary_0.5_up <- as.factor(ifelse(predict_binary_up>0.5,1,0))
confusionMatrix(predict_binary_0.5_up,validation_df_y_bin)
```

```{r}
LogLoss(y_true = y_bin_unf, y_pred =predict_binary_up)
```
```{r}
bin_auc_up <- AUC(predict_binary_up, validation_df_y_bin)
bin_auc_up
```

For this business problem, we think it would be beneficial to have a higher sensitivity, so the model with up sampling is preferred (sensitivity of 0.50676). It should be noted that the precision is 0.201 compared to 0.285 so this is a tradeoff we need to make.

```{r}
predict_binary_0.15_up <- as.factor(ifelse(predict_binary_up>0.15,1,0))
confusionMatrix(predict_binary_0.15_up,validation_df_y_bin)
```
```{r}
varImp(fit_training_b_up)
```

We can see that features associated with the user’s interaction levels with the website drove conversion factors. We would anticipate that customers who are interacting by way of page views, visits, and clicks would be most likely to respond.

```{r}
#write.csv(validation_df_y_bin, "y_validation.csv")
#write.csv(predict_binary_up, "rf_bin_upsample.csv")
#write.csv(predict_binary, "rf_bin.csv")
```


Below, we repeat the exercise for XGBoost and Ensemble models.

### XGBoost Model

```{r}
xgb.grid.binary <-  expand.grid(nrounds = 50,eta = c(0.1,0.01,0.001)  # hyperparameter: learning rate
                         ,max_depth = c(2,4,6,8,10), # hyperparameter: size of a tree in each boosting iteration
                         gamma = 1, colsample_bytree=1, min_child_weight=1, subsample=1)

xgb.fitControl.binary <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE,
                          classProbs = TRUE,
                           summaryFunction=multiClassSummary ) # try upsampling and downsampling**

 

xgb.fit.binary = train(
  x = train_df,
  y = train_df_y_bin,
  method = 'xgbTree',
  metric='logLoss',
  tuneGrid = xgb.grid.binary,
  trControl = xgb.fitControl.binary
)

 

print(xgb.fit.binary)
```

```{r}
predict_binary_xgb <- predict(xgb.fit.binary, validation_df, type = "prob", verbose=TRUE)$response
predict_binary_0.5_xgb <- as.factor(ifelse(predict_binary_xgb>0.1,1,0))
confusionMatrix(predict_binary_0.5_xgb,validation_df_y_bin)
```


### XGBoost Binary with Hyperparameter tuning using gridsearch & Upsampling

```{r}
xgb.grid.binary.up <-  expand.grid(nrounds = 50,eta = c(0.1,0.01,0.001)  # hyperparameter: learning rate
                         ,max_depth = c(2,4,6,8,10), # hyperparameter: size of a tree in each boosting iteration
                         gamma = 1, colsample_bytree=1, min_child_weight=1, subsample=1)

 
xgb.fitControl.binary.up <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE,
                          classProbs = TRUE,
                          sampling="up",
                           summaryFunction=multiClassSummary ) # try upsampling and downsampling**

 

xgb.fit.binary.up = train(
  x = train_df,
  y = train_df_y_bin,
  method = 'xgbTree',
  metric='logLoss',
  tuneGrid = xgb.grid.binary.up,
  trControl = xgb.fitControl.binary.up
)

 

print(xgb.fit.binary.up)
```

#### Analyzing REsults and Performance Metrics for Binary Classification (XGBoost) Models
```{r}

y_bin_unf <- unfactor(validation_df_y_bin)

# Predict probabilities and assess based on validation dataset
predict_training_xgb_1 <- predict(xgb.fit.binary, validation_df, type="prob")
predict_training_xgb_1_f <- as.factor(ifelse(predict_training_xgb_1[,1]>0.1,1,0))
confusionMatrix(predict_training_xgb_1_f,validation_df_y_bin)
pred1 <- predict_training_xgb_1[,1]

# Predict probabilities and assess based on validation dataset

predict_training_xgb_up <- predict(xgb.fit.binary.up, validation_df, type="prob")
predict_training_xgb_up_f <- as.factor(ifelse(predict_training_xgb_up[,1]>0.25,1,0))
confusionMatrix(predict_training_xgb_up_f,validation_df_y_bin)
pred2 <- predict_training_xgb_up[,1]


y_bin_unf <- unfactor(validation_df_y_bin)

#Log Loss Scores
LogLoss(y_true = y_bin_unf, y_pred =predict_training_xgb_1[,1] )
LogLoss(y_true = y_bin_unf, y_pred =predict_training_xgb_up[,1])

#AUC
bin_auc1 <- AUC(predict_training_xgb_1[,1], validation_df_y_bin)
bin_auc1

#AUC - model with usampling
bin_auc2 <- AUC(predict_training_xgb_up[,1], validation_df_y_bin)
bin_auc2

# Recall-Precision curve      
pred1f <- prediction(pred1, validation_df_y_bin)
RP.perf <- performance(pred1f, "prec", "rec");
plot (RP.perf);
# ROC curve
ROC.perf <- performance(pred1f, "tpr", "fpr");
plot (ROC.perf);


# Recall-Precision curve   
pred2f <- prediction(pred2, validation_df_y_bin)
RP.perf <- performance(pred2f, "prec", "rec");
plot (RP.perf);
# ROC curve
ROC.perf <- performance(pred2f, "tpr", "fpr");
plot (ROC.perf);


#write.csv(predict_training_xgb_1,"C:\\Users\\panes\\Desktop\\Jaspal\\Queens MMA\\MMA 831 - Marketing Analytics\\Assignment 1\\IMB 623 VMWare- Digital Buyer Journey\\pred1.csv")
#write.csv(predict_training_xgb_up,"C:\\Users\\panes\\Desktop\\Jaspal\\Queens MMA\\MMA 831 - Marketing Analytics\\Assignment 1\\IMB 623 VMWare- Digital Buyer Journey\\pred2.csv")
```


### Ensemble Modelling 
Loading vector of predictions from rf and xgboost 
```{r}
rf_bin <-read.csv("C:\\Users\\hp\\Documents\\Smith School of Business\\M831-Marketing Analytics\\Assignment 1 Mid Term\\Trained Models\\rf_bin.csv", na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
```

```{r}
rf_bin_upsample <-read.csv("C:\\Users\\hp\\Documents\\Smith School of Business\\M831-Marketing Analytics\\Assignment 1 Mid Term\\Trained Models\\rf_bin_upsample.csv", na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
```


```{r}
xgb_reg <-read.csv("C:\\Users\\hp\\Documents\\Smith School of Business\\M831-Marketing Analytics\\Assignment 1 Mid Term\\Trained Models\\xgb_reg.csv", na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
```

Loading Validation vector file into R 
```{r}
y_validation <-read.csv("C:\\Users\\hp\\Documents\\Smith School of Business\\M831-Marketing Analytics\\Assignment 1 Mid Term\\Trained Models\\y_validation.csv", na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
```

Ensemble Model Data Frame containing vector of probabilites form all three models 
```{r}
ensemble_probs<-data.frame(rf=rf_bin[,2], rf_upsample=rf_bin_upsample[,2],xgb=xgb_reg[,2])

```

### Average Ensemeble Model 
```{r}
ensemble_probs$Avg<-(ensemble_probs$rf+ensemble_probs$rf_upsample+ensemble_probs$xgb)/3



```

#### Building Confusion Matrix 
```{r}

ensemble_probs$Avg1<-(ifelse(ensemble_probs$Avg>0.10,"1","0"))
ensemble_probs$Avg1<- as.factor(ensemble_probs$Avg1)


LogLoss(y_true = y_validation$x, y_pred =ensemble_probs$Avg)

y_validation$x1 <- as.factor(y_validation$x)

#Confusion Matrix 

cmatrix<- confusionMatrix(ensemble_probs$Avg1,y_validation$x1,positive="1")
cmatrix


```

### Ensemble Average Voting Model 
```{r}
i=0.10
ensemble_classification<-rep("1",50006)
ensemble_probs$rf_class <- 1
ensemble_probs$xg_class <- 1
ensemble_probs$lr_class <- 1
  
ensemble_probs$rf_class[ensemble_probs$rf<i]=0
ensemble_probs$xg_class[ensemble_probs$rf_upsample<i]=0
ensemble_probs$lr_class[ensemble_probs$xgb<i]=0
  
ensemble_probs$sum <- ensemble_probs$rf_class + ensemble_probs$xg_class + ensemble_probs$lr_class
  
ensemble_classification[ensemble_probs$sum<2]="0" 

ensemble_classification<-as.factor(ensemble_classification)
  
cmatrix <- confusionMatrix(ensemble_classification,y_validation$x1, positive="1") #Display confusion matrix. Note,
cmatrix
```



